\section{Realisation}
\subsection{Engineering work}
My work was split into many different parts. I made a \brand{GitHub} deposit : \url{https://github.com/jcelerier/spectral-subtraction} in which there are folders that correspond to these parts.

\subsubsection{Implementation of the algorithms}
This part will refer to the folder \texttt{libnoisered}. 

\paragraph{Technologies used}
As soon as I could, I tried to make the algorithms as a separate library, which has for sole dependencies \ac{FFTW} and \brand{CWTLIB++}. A corresponding \brand{Doxygen} documentation can be generated inside this folder, or by calling \texttt{make doc} on the root of the repository.
The library is of course written in \brand{C++}, and I tried to adhere to the latest standard, \brand{C++11}, as much as possible. For this reason, only fairly recent compilers will work : \brand{gcc-4.7} and \brand{Visual C++ 2013} (still in a release candidate at the time of this writing).
I tested compilation with a massive set of warnings to ensure the less bugs possible (cf. Appendix \ref{appendix2})

\subparagraph{Trying to be general}
I tried to make some of my code very generic, very performant, and the most parallel-friendly possible so that I could reuse this if the need would show up.
One good example would be my implementation of the MapReduce algorithm, very useful in a lot of mathematical computations.
For instance, a common value to compute in the library was the sum of the squares : 

\begin{align*}
\displaystyle\sum_{\displaystyle i=0}^{\displaystyle i<N} \displaystyle x^2
\end{align*}

I came up with the following way to achieve this : 
\begin{lstlisting}[caption=math\_util.cpp]
template <typename T, typename InputIterator, typename MapFunction, typename ReductionFunction>
T mapReduce_n(const InputIterator in,
			  const unsigned int size,
			  const T baseval,
			  const MapFunction map,
			  const ReductionFunction reduce)
{
	T val = baseval;

	#pragma omp parallel
	{
		T map_val = baseval;

		#pragma omp for nowait
		for (auto i = 0U; i < size; ++i)
			map_val = reduce(map_val, map(*(in + i)));

		#pragma omp critical
		val = reduce(val, map_val);
	}

	return val;
}
\end{lstlisting}
\newpage
The usage can be as follows, for instance with a complex number : 
\begin{lstlisting}
double val = MapReduce_n(input_vector, size, 0.0, [] (complex val)
    {
        return complex.x * complex.x + complex.y * complex.y;
    }, 
    std::plus<double>());
\end{lstlisting}

It could be made even faster when writing a specialisation for the $std::plus$ operator.

For instance, a simple loop :
\begin{lstlisting}
#pragma omp parallel for
for (auto i = 0U; i < size; ++i)
    val += in[i].x * in[i].x + in[i].y * in[i].y;
\end{lstlisting} 
takes $2.6$ seconds while my method takes $0.4$ seconds, and the specialized one $0.2$.

\subparagraph{Choice of \ac{FFTW}} I choose to use \ac{FFTW} because it seems that it is one of the best performing Fourier Transform, apart from implementations which are devised specially towards a particular \brand{CPU}: \url{http://www.fftw.org/speed/}.
It also supports the \brand{NEON} vectorial instructions of the \brand{ARM} processor, which can provide a performance boost.
This is an area where performance is critical : I did some profiling of the software during my internship, and most of the time is spent in the \ac{FFT} computation.
\subparagraph{Overlap-add technique}
One of the technique used here is the overlap-add technique. Instead of performing a Fourier transform on a data-filled array, the input array is twice the data size, and its second half is filled by zeros.
After the subtraction is applied, the output array's first half is added to the previous array's last half, and so on. This is the only point on the algorithms that can't be threaded easily.
Overlap-add method can be enabled or disabled. If it is disabled, a simple rectangular window is used.

\paragraph{Simple noise estimation}
The most basic noise estimation algorithm would be to take the first frame of the signal, assume nobody is talking, and use it as a noise estimation.
However, since the algorithm is meant for real-time applications which could last for some minutes or even hours, the background noise might change.

So I decided to improve it by computing the \ac{RMS} level for each frame, and replacing the noise estimation with a new frame if the level is under $102\%$ of the current level.
\newpage
\paragraph{Spectral Subtraction}
The numerical algorithms are most of the time very straightforward.

\begin{algorithm}
 \SetAlgoLined
 \KwData{A signal spectrum X, a noise estimation Z}
 \KwResult{A new signal X' with less noise}

 \ForAll{bands $i$ in the spectrum}{
	$ t_0\gets \power(X_i)$ \\
	$ t_1 \gets t0 - \alpha * Z(i)$ \\
	$ t_2 \gets \beta * t0$ \\
	$ \mathrm{mag} \gets \sqrt{\max(t1, t2)}$ \\
	$ \mathrm{ph} \gets \phase(X_i)$ \\
	$ X'_i \gets \recompute(mag, ph)$ 
	\tcp{Spectrum is given in complex format}
 }
 \caption{Simple spectral subtraction (\texttt{subtraction/simple\_ss.cpp})}
\end{algorithm}

Whenever I could, I tried to use \brand{C++ STL} algorithms, like \texttt{std::transform}, or \texttt{std::accumulate}.
There are several advantages : 
\begin{itemize}
\item They make the code very concise and readable. For instance, here is the code that computes the power for every band of a spectrum array, in complex format : 
\begin{lstlisting}[caption=math\_util.cpp]
void computePowerSpectrum(const std::complex<double> * in, 
	double * powoutput, 
	unsigned int size)
{
	std::transform(in, in + size, powoutput, CplxToPower);
}
\end{lstlisting}

Where \texttt{CplxToPower} is a wrapper : 
\begin{lstlisting}[caption=math\_util.cpp]
double CplxToPower(const std::complex<double> val)
{
	return std::norm(val);
}
\end{lstlisting}

On a side note, I also tried to use lambda-expressions whenever I could because common compilers seems to optimize them better than ordinary functions\cite[p.~213]{cppstl}, and here performance is critical:

\begin{lstlisting}[caption=estimation/wavelets/cwt\_wavelet\_estimator.cpp]
ArrayValueFilter lowCeiling([&](unsigned int i, unsigned int j)
	{ arr[i][j] = (arr[i][j] > ceil) ? arr[i][j] : 0; });
\end{lstlisting}


\item The \brand{C++} standard imposes a maximal complexity for these algorithms. For instance, \texttt{std::max} must be in $\theta(n)$ (courtesy of \url{www.cplusplus.com}).
\item They present the advantage of having parallel versions, when using \brand{GCC}. These versions are enabled with a single \brand{G++} flag : \texttt{D\_GLIBCXX\_PARALLEL}. This way, they can be easily disabled for the embedded board, where it is inefficient because it is single-threaded, while they can be enabled when doing tests on the computer. 
\end{itemize}


\paragraph{First organisation}
At first, when there were only a few algorithms to code, it seemed logical to have a class only for noise estimation algorithms, and a class only for subtraction algorithms. However, some algorithms like the one proposed by Rainer Martin\cite{martin2001noise} uses a lot of static variables and tables, because data has to be kept from a frame to another. 
Here was the first class diagram : 
\begin{figure}[H]
\begin{center}
\hspace{-11em}
\includegraphics[scale=0.5]{old.pdf}
\caption{First organization of the library}
\label{diag_api_chords}
\end{center}
\end{figure}
The problem is that the data and the algorithms were intertwined. This could have caused problems when trying to thread. Also, the \texttt{SpectralSubtractor} and \texttt{NoiseEstimator} had to be friend with \texttt{SubtractionData}, which held many of the parameters required to perform the algorithms, like the number of iterations, etc.
Finally, the user had to manipulate two different objects, which causes unneeded clutter in the code.
\paragraph{Mid-internship reorganisation}
I then decided to refactor my code, by taking a modular approach, that would be more logical.
The user mostly interacts with the \texttt{SubtractionManager} object. This object holds : 
\begin{itemize}
\item The audio data.
\item Methods for loading an audio buffer, or a raw audio file.
\item A method to read parameters from a configuration file.
\item Methods to manage the Fourier transform.
\item Handlers to call if there is a change of data, or FFT size.
\item Methods to set general parameters, like the number of iterations, or the usage of the overlap-add method.
\item Two very important objects, of base class \texttt{Estimation} and \texttt{Subtraction}, and the associated getters and setters.
\item An object to manage the Fourier Transform, of base class \texttt{FFTManager}.
\end{itemize}
The \texttt{Estimation} and \texttt{Subtraction} objects hold the algorithm implementation. This reduces memory consumption, because only the wanted algorithm and needed implementation details are loaded, versus the previous approach where every algorithm had to be initialized. In order to permit a good memory management, I used a \brand{C++11} smart pointer \texttt{std::shared\_ptr} to keep them.

An example of initialization would be : 
\begin{lstlisting}[caption=Example initialization]
SubtractionManager s_mgr(fft_size, sampling_rate);
SimpleSpectralSubtraction* sub_algo = new SimpleSpectralSubtraction(s_mgr);
// Here we set algorithm-specific parameters.
// For instance they are not present in GeometricSpectralSubtraction class
sub_algo->setAlpha(data->alphaBsc); 
sub_algo->setBeta(data->betaBsc);
s_mgr.setSubtractionImplementation(sub_algo);
s_mgr.setEstimationImplementation(new MartinNoiseEstimation);
\end{lstlisting}

The \texttt{Estimation} and \texttt{Subtraction} objects are functors. I choose to build them with a reference to the \texttt{SubtractionManager} object because had not I done this, I would have to add at least two parameters to most methods in \texttt{Estimation} and \texttt{Subtraction}, one to get the sampling rate, and one to get the size of the arrays, because they are both needed most of the time.

Finally, by the end of the internship I tried to make my library independent from the implementation of the \ac{FFT}. I had originaly used the \texttt{fftw\_complex} type everywhere a complex number was required. But I thought it would be better to keep the library ahead of this implementation detail, so I changed it to use the \texttt{std::complex} class.

I also made a wrapper class, \texttt{FFTManager}, which is subclassed by \texttt{FFTWManager}. I had to find the minimal interface required to perform the \ac{FFT}. This class also holds the result of the Fourier transform.

There is an important use for this abstraction. While doing profiling, I found that the \ac{FFT} was still the part of the program where the most time was spent. \ac{FFTW} might be quite fast, but it can be outperformed by architecture-specific implementations. For instance, \brand{Texas Instruments} ships some code to perform a very fast \ac{FFT} using the \ac{DSP} of the \brand{BeagleBoard}. As well, \brand{Intel} ships the \brand{Math Kernel Library\textregistered}, specialized for use with its processors since it uses custom extensions like \ac{AVX}.

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.4]{base.png}
\caption{Current organization of the library. Not all implementation details present.}
\label{diag_api_chords}
\end{center}
\end{figure} 

The \texttt{Subtraction} class is little more than an interface, with only the constructors and destructors not being pure virtual. However, the \texttt{Estimation} class needs to keep track of the current noise estimation (in the form of an array of \texttt{doubles}). For this, I choose to use the \textbf{Template method pattern}:

Calling \texttt{Estimation::onFFTSizeUpdate()} or \texttt{Estimation::onDataUpdate()} will perform some actions needed for the noise estimation array, like resizing it, and make a subsequent call to \texttt{Estimation::specific\_onFFTSizeUpdate()} or \texttt{Estimation::specific\_onDataUpdate()}, which are left pure virtual in the base class. It is up to the real algorithms to implement them.
\newpage
\paragraph{Core algorithm}
Finally, a very important algorithm is \texttt{SubtractionManager::execute()} because it is the one that gets called when the data and parameters are ready.

Thanks to the care taken in the design, the code is very short and extremely generic: 
\begin{lstlisting}[caption=\texttt{SubtractionManager::execute()} in subtraction\_manager.cpp]
for (auto iter = 0U; iter < iterations(); ++iter)
{
	for (auto sample_n = 0U; sample_n < getLength(); sample_n += getFrameIncrement())
	{
		copyInput(sample_n);
		forwardFFT();

		// Noise estimation (functor)
		estimation(spectrum());

		// Spectral subtraction (functor)
		subtraction(spectrum(), estimation->noisePower());

		backwardFFT();
		copyOutput(sample_n);
	}
}
\end{lstlisting}

\subparagraph{Approach taken for the iterations management}
As it can be seen on the previous graph, the first loop is the iterations loop. One of the problems I was struck with was how to make the iterations work with the noise estimation ? 

On the case of a file, \texttt{sample\_n} will begin at the beginning of the file, and end at the end of the file, so it will be a non-problem : every iteration will run the whole file.

However, what about the real time signals ?
Since the estimation algorithms can reestimate the noise, and since the noise won't be similar between one and twenty iterations, what happens when we pass on the next frame on the case of a real-time signal ? For instance, if it is a frame entirely made of speech, there can't be reestimation, but the current noise estimation isn't for the same kind of noise as the one in the current unprocessed frame.

This is why the Martin algorithm is needed in case of real time signals : it can estimate noise even if there is only speech. The only other way to solve this would have been to wait for the speakers to finish their speech, process the whole speech and send it to Julius, but it would not be real-time anymore. So I prefered to use the callback approach, since it would induce far less latency.

\subsubsection{Test application}
This part will refer to the folder \texttt{denoiseGUI}. 
\paragraph{Technologies used}
The application is fully developed using the \brand{Qt} framework, so that it can work easily on different environments (all the major desktop operating systems). It has only been tested on the \brand{Qt5} version.

\paragraph{Features of the software}
I had to make this application to compare the algorithms, and later, to perform batch processing on either a lot of files or a lot of parameters.
It can change all the relevant algorithms and parameters, load audio files and playback them before and after processing, and computes relevant evaluation parameters, the noise reduction ratio and speech distortion ratio\cite{horii2013musical}.
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.75]{testapp1.png}
\caption{Test application, upon opening}
\label{diag_api_chords}
\end{center}
\end{figure}

\subparagraph{Batch processing} It is enabled in two different ways. 
Noise reduction ratio and speech distortion ratio are both computed, if the correct files are loaded.
One can either:
\begin{itemize}
\item process a single file with ranges of parameters, for instance with $\alpha$ going from $1$ to $3$ by a step of $0.5$ and  $\beta$ going from $0.02$ to $0.08$ by a step of $0.01$, when using standard spectral subtraction. There can be a comparison between different algorithms, too.
\item process all the raw files in a subdirectory with a single set of parameters.
\end{itemize}

The output is put in a table which has copy-paste abilities, to put the results on a spreadsheet software, like \brand{LibreOffice Calc} or \brand{Microsoft\textregistered Excel\texttrademark}. This is very useful to plot graphs, for instance.
To copy the data, the standard keyboard shortcut is used : \texttt{CTRL-A} to select everything and \texttt{CTRL-C} to copy.

\subsubsection{Making the processed signal listenable}
I had to add an audio output which would play the modified signal in real time on the \brand{BeagleBoard} implementation. However, I had some problems with this at first. \brand{Julius} internally uses \ac{ALSA} for its microphone input, so at first, I thought I should use \ac{ALSA} too, to manage the output. However, while learning, I discovered than adding full-duplex operation (both input and output) was not well supported, and that I would have to modify the source code of \brand{Julius}, which I wanted to avoid the most to preserve compatibility with future updates.

My solution was to use another audio interface wrapper, which is part of the \brand{Qt} toolkit :\\ \texttt{QAudioOutput}. I already knew how to use it since it was used in the testing application so the implementation was quick. It adds the \brand{Qt} dependency to the project, but it is available on all major platforms, including the \brand{BeagleBoard}. One of the concerns was with the buffer size : I had to dive a bit deeply inside \brand{BeagleBoard} forums to find that the optimized buffer size for the audio chipset would be $1024$ samples. This might vary on other devices, however, so the code might need to change on this point if the software was to be ported.

\subsubsection{Implementation of an English acoustic and language model}
This part will refer to the folder \texttt{timit\_htk}. 
\paragraph{Technologies used}
I used mainly \ac{HTK}, and made a script using \brand{Python}, plus some calls to standard Unix command-line utilities, like \brand{awk} and \brand{sed}. \brand{SOX} is also used to perform resampling.
\paragraph{Explanation of the method used}
There are only four files in this folder.
\begin{itemize}
\item \texttt{htk.py} is the main python script for the generation of the models.
\item \texttt{configuration.py} keeps some useful text data and configuration values that can be tweaked.
\item A convenience \texttt{makefile}, whose \texttt{clean} command is very useful because of the number of temporary files generated by the process.
\item \texttt{CORRECTED\_TIMITDICT.TXT} is a dictionary used by the generation of the language model.
\item Not present in the repositories because of the copyright is the \ac{TIMIT} folder, which has to be put here exactly as it is on the CD-ROM prior to generation of the model.
\end{itemize}

Prior to everything, the corpus must be resampled : this is achieved by calling the \texttt{Resample(path, dirs)} function. It has to be called only one time, because else the raw audio files get corrupted by \brand{SOX}. For this reason, it is commented in the code
If there is a need to re-run the scripts, the call to \texttt{Resample} has to be commented again, and the main script can run normally.

There are many small steps involved. The process mostly follows the steps given by the \brand{HTK Book}\cite{htkbook}, the reference book on the subject.
The auxiliary functions are mostly here to convert the \ac{TIMIT} training data into data compatible with \ac{HTK}.

\paragraph{The TIMIT corpus}
The corpus is a set of sentences pronounced by different speakers, which all have a different accent. There is also a dictionary.

There are four kind of files, one audio and three textual.
The textual files always follow the same convention : a line begins with two numbers, which are the starting and ending millisecond of the content of the line, which is located afterwards:
\begin{itemize}
\item \texttt{.raw} files : audio recordings of the speakers.
\item \texttt{.txt} files : sentence-level transcription of the recordings. Exemple :
\begin{figure}[h]
\centering
\begin{verbbox} 0 46797 She had your dark suit in greasy wash water all year. \end{verbbox}
\theverbbox
\end{figure}
\item \texttt{.wrd} files : word-level transcription:
\begin{figure}[h]
\centering
\begin{verbbox}3050 5723 she
5723 10337 had
9190 11517 your
11517 16334 dark 
...
\end{verbbox}
\theverbbox
\end{figure}
\item \texttt{.phn} files : phoneme-level transcription:
\begin{figure}[h]
\centering
\begin{verbbox}0 3050 h#
3050 4559 sh
4559 5723 ix
5723 6642 hv 
...
\end{verbbox}
\theverbbox
\end{figure}
\end{itemize}

\ac{HTK}, on the other part, requires a particular format of segmentation file to train the model, where all the data from all the files is concatenated into a single file. The times must be specified in samples :
 
\begin{figure}[h]
\centering
\begin{verbbox}
#!MLF!#
"*/TRAIN_DR3_MTPG0_SA1.lab"
2018750 3625000 she
3625000 6005000 had 
6005000 6725000 your 
6725000 9686250 dark 
\end{verbbox}
\theverbbox
\caption{Beginning of \texttt{wordTRAIN.mlf}, generated by \texttt{htk.py}}
\end{figure}

Likewise, there are many other textual conversions to perform, which are mostly found in the separate functions of the script.

\subsubsection{Evaluation}
\paragraph{Signal-level evaluation}
There are two common metrics\cite{horii2013musical} to evaluate the quality of the noise reduction : the \ac{NRR} and \ac{SDR}.
\begin{itemize}
\item The \ac{NRR} compares the signal before and after application of the noise reduction algorithm. It is an indicator of how much of the noise have been reduced: it is better when it is superior to $0$. However, if it is too high, the signal might not be recognizable at all anymore.
\item The \ac{SDR} compares a noiseless signal to the noise-removed signal. Hence, it is not useable in a real situation : the noise has to be applied artificially to some source signal which has to be kept somewhere. It is an indicator of how much of the vocal part of the signal has been damaged by the algorithm: it is better when it is close to $0$.
\end{itemize}

During my internship, I compared many times the different algorithms and parameters. Thanks to the extensive textual resources of the lab, I could compare my results with reference scientific results, which allowed me to perform some debugging, and to be sure of my implementation.
\paragraph{Word-level evaluation}
To evaluate the performance on word-level, it is necessary to use the speech recognition software (\brand{Julius}) : we give some audio file with sentences to the software and it outputs the text it recognized. I made a script (\texttt{output/execute\_speech\_recog.py}) that takes a given set of sentences, and the same sentences processed with different kinds of noise, and finally calls \brand{Julius} with multiple algorithms for each of these sets of files. The script then computes a word error rate by comparing the original sentences and computed sentences. 
The algorithms are as follows : 
\begin{itemize}
\item Bypass : no algorithm
\item Std : Simple estimation, standard spectral subtraction, $\alpha = 3$, $\beta = 0.01$, one iteration.
\item EqualLoudness : Simple estimation, EL-SS, $\alpha = 3$, $\beta = 0.8$, $\alpha_{wt} = 0.02$, $\beta_{wt} = 0.005$, twenty iterations.
\item Ga : Simple estimation, geometric algorithm, one iteration.
\item GaMartin : Martin estimation, geometric algorithm, one iteration.
\item GaMartin2 : Martin estimation, geometric algorithm, two iterations.
\end{itemize}Then, it averages the results for each set.

I chose to use twenty sentences from the \ac{JNAS} speech corpus, to which I applied all the noises with a script called \texttt{ApplyNoiseToAudio}, located in the folder of the same name, that I did in \brand{C\#} for the sake of learning the basics of the language.
This script also computes files for different signal-to-noise ratios : I chose ratios of $0, 10, 20, 30, 50$ decibels.

Computing all the values took about seven hours on an \brand{Intel Core i7} processor.

We still get a lot of results, that I averaged in a spreadsheet :

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.60]{images/worderrorratenoise.png}
\caption{Error rate for white noise with different algorithms.}
\end{center}
\end{figure}


\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.7]{images/snr.png}
\caption{Error rate for all noises with a 10db SNR.}
\end{center}
\end{figure}


\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.7]{images/snr30.png}
\caption{Error rate for all noises with a 30db SNR.}
\end{center}
\end{figure}

One first thing to notice is that using noise reduction is not always wise : on the first and third graph we can see that the bypassed error rate is better than the non-bypassed. 

However we can also see how the signal-to-noise ratio affects the results : the geometric algorithms will perform better than the others on low SNR. So the algorithms should only be applied when there is a lot of noise.
\subsection{Research work}
\subsubsection{First idea to improve spectral subtraction}
As said before, the problem with spectral subtraction is that it generates musical tones which can be a problem for the listener, or for the speech recognition software.

So I decided to try to find a way to reduce, if not suppress, these musical tones.
For this, I would have to recognize them efficently.

Following the advices of Mr. Nishiura, I made a research map to explain the process I followed : 

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.35]{images/mindmap.png}
\caption{Research map}
\label{diag_api_chords}
\end{center}
\end{figure}

So, my first idea was to find a new way to detect musical tones in a noise-reduced signal.
To do this, I thought about defining them, and it occured to me that a time $\longleftrightarrow$ frequency plane would be a good way to try to understand them.

I thought of two ways of building one : either by using a lot of FFTs one after each other, or either by doing a continuous wavelet transform. However I was not sure of the capacity of the first method to solve the problem, so I tried to go with the second one.

I had first to learn how to use the continuous wavelet transform, and to choose a good wavelet to perform it. After some experimentation with Matlab, it seemed that the most efficient wavelet for what I was trying to achieve was the \textit{Morlet} wavelet. 

Then, I tried to find a fast and portable implementation of a wavelet transform : I found \brand{cwtlib++}, a \brand{C++} wavelet transform library, with no dependencies.

\paragraph{Finding the musical tones}
My global idea to reduce the noise was as follows : 
\begin{enumerate}
\item Perform a spectral subtraction on a FFT frame $X$ with a noise estimation $E$ into $X'$ : $X'_i = X_i - E_i$.
\item Estimate the quantity of musical tones $M$ in each frequency band of $X$.
\item Substract this quantity to $E$ into a new, temporary noise estimation : $E'_i = E_i - M_i$.
\item Re-perform another spectral subtraction : $X_i - E'_i$.
\end{enumerate}
This is because musical tones are caused by over-subtraction in some areas of the signal, because the noise estimation doesn't always corresponds to the noise being really present. 

To estimate the musical tones, I came across multiple heuristics induced by observation of musical tones generated in real frames.

\begin{itemize}
\item In one FFT frame, there will be different groups of musical tones, but for each of these groups, every musical tone is around the same frequency.
\item The musical tones are louder in volume than the residual ambient noise, but quiter than the speech material.
\item In a continuous wavelet transform, they mostly have simple shapes.
\end{itemize}
Some pictures can be seen in the poster in appendix \ref{appendix1}.
Using these algorithms, I made the \texttt{CWTNoiseEstimator} class that tries to implement these ideas. I made a shape recognition algorithm that plots the potential musical tones according to the previous heuristics, and computes the parameters of the musical tone (frequency, amplitude...). Then, I make an average for every band of frequency, to create $M$. Unfortunately, as it can be seen on the results in the poster, there is not much gain finally.
\paragraph{Kansai joint speech seminar}
I participated in a conference held in a university close to Ritsumeikan. I had to make a poster which will be shown in \ref{appendix1}.
The main theme of this seminar is research about audio, with a little emphasis on speech. It was held on \formatdate{20}{07}{2013}. The seminar was in three rooms, two were mostly about signal processing, whlie another was more about language study on a semantic level. The complete list of the presentations, mostly in Japanese, is here : \url{http://nlp.i.ryukoku.ac.jp/gakkai/godozemi/201307/index.html}

Students and teachers from different Kansai universities, and sometimes other foreign student would comment on the poster of every student.
(Note : the Kansai (\begin{CJK}{UTF8}{min}関西地方\end{CJK}) is a region of Japan which encompasses Osaka, Kyoto, Nara...).

\subsubsection{Second idea to reduce noise}
After this, I started to try to think about other ways to reduce noise, and I thought about using some parts of what I had heard of during my previous internship : using reinforcment learning to set the standard spectral subtraction parameters $\alpha$ and $\beta$ in real time, according to the noise, because some of my measurements shown that some couples would work better with some kinds of noises than other couples.

My idea was as follows : 
\begin{enumerate}
\item Using different kinds and families of noises, create a library of "situations" that would serve as a starting point.
\item When running in a new environment the software chooses quickly a good couple $(\alpha, \beta)$ but the learning algorithm keeps running to reduce further the amount of noise.
\end{enumerate}

The learning algorithms need a reward, so that they learn which changes give a reward and which changes give a malus.
Since we need to reduce noise, I thought about comparing the \ac{NRR} with the new parameters, with the \ac{NRR} with the previous parameters.
I was confronted to two problems : 
\begin{itemize}
\item The most common algorithms, \brand{Q-Learning}\cite{watkins1992q} and \brand{SARSA}\cite{rummery1994line}, are made for discrete values while  $(\alpha, \beta)$ is continuous.
\item These algorithms are meant to stop when some "action" is performed, like for instance going from $A$ to $B$ while dodging a pit, while here, we need an algorithm that keeps performing all the time.
\end{itemize}

I put my work in two folders : \texttt{libnoisered/learning} and \texttt{libnoisered/subtraction/learning\_ss}.

However, due to lack of time and knowledge about machine learning, I couldn't finish working on it.
